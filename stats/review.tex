\documentclass[12pt]{article}
\textwidth = 6.7 in
\textheight = 9.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.2 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

% ***********************************************************
% *********************** HEADER  ***************************
% ***********************************************************

\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}	% Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage{multicol} % Allows for multiple columns
\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
% Sets margins and page size
\pagestyle{empty} % Removes page numbers
\makeatletter % Need for anything that contains an @ command 
\renewcommand{\maketitle} % Redefine maketitle to conserve space
{ \begingroup \vskip 10pt \begin{center} \large {\bf \@title}
    \vskip 10pt \large \@author \hskip 20pt \@date \end{center}
  \vskip 10pt \endgroup \setcounter{footnote}{0} }
\makeatother % End of region containing @ commands
\renewcommand{\labelenumi}{(\alph{enumi})} % Use letters for enumerate
% \DeclareMathOperator{\Sample}{Sample}
\let\vaccent=\v % rename builtin command \v{} to \vaccent{}
\renewcommand{\v}[1]{\ensuremath{\mathbf{#1}}} % for vectors
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} 
% for vectors of Greek letters
\newcommand{\uv}[1]{\ensuremath{\mathbf{\hat{#1}}}} % for unit vector
\newcommand{\abs}[1]{\left| #1 \right|} % for absolute value
\newcommand{\avg}[1]{\left< #1 \right>} % for average
\let\underdot=\d % rename builtin command \d{} to \underdot{}
\renewcommand{\d}[2]{\frac{d #1}{d #2}} % for derivatives
\newcommand{\dd}[2]{\frac{d^2 #1}{d #2^2}} % for double derivatives
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} 
% for partial derivatives
\newcommand{\pdd}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
% for double partial derivatives
\newcommand{\pdc}[3]{\left( \frac{\partial #1}{\partial #2}
  \right)_{#3}} % for thermodynamic partial derivatives
\newcommand{\ket}[1]{\left| #1 \right>} % for Dirac bras
\newcommand{\bra}[1]{\left< #1 \right|} % for Dirac kets
\newcommand{\braket}[2]{\left< #1 \vphantom{#2} \right|
  \left. #2 \vphantom{#1} \right>} % for Dirac brackets
\newcommand{\matrixel}[3]{\left< #1 \vphantom{#2#3} \right|
  #2 \left| #3 \vphantom{#1#2} \right>} % for Dirac matrix elements
\newcommand{\grad}[1]{\gv{\nabla} #1} % for gradient
\let\divsymb=\div % rename builtin command \div to \divsymb
\renewcommand{\div}[1]{\gv{\nabla} \cdot #1} % for divergence
\newcommand{\curl}[1]{\gv{\nabla} \times #1} % for curl
\let\baraccent=\= % rename builtin command \= to \baraccent
\renewcommand{\=}[1]{\stackrel{#1}{=}} % for putting numbers above =
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
% \def\rmdefault{bch} % Use Charter for main text font.

% ***********************************************************
% ********************** END HEADER *************************
% ***********************************************************
% =========================================================
\begin{document}

\begin{center}
  {\LARGE
    \textbf{Stats Review}\\
    \normalsize
    \textnormal {by Marcell Vazquez-Chanlatte}
    \\[1ex]}

  
\end{center}

% ------------------------------------------------------------
\textbf{Counting:}
\begin{enumerate}
\item Counting $n$ tasks($m_i$) in order:
  \begin{equation}
    \textrm{Mult Principle:  } \abs{A}= \displaystyle\prod_{i = 0}^{n}m_i
  \end{equation}
\item The number of ways to do $n$ non-overlapping tasks:
  \begin{equation}
    \textrm{Add Principle: } \abs{A} = \sum_{i=0}^n m_i
  \end{equation}
\item Choose $k$ ordered elements of $A, \abs{A}=
  n$:
  \begin{equation}
    \textrm{Permutations: } P(n,k) = \frac{n!}{(n-k)!} 
  \end{equation}
\item The number of unique non-ordered lists:
  \begin{equation}
    \textrm{Combinations: } C(n,k) = \binom{n}{k} = \frac{P(n,k)}{P(k,k)} =
    \frac{n!}{(n-k)!k!}
  \end{equation}
\item Size of the set($X$) of all possible events in set
  $A$, with size $n$:
  \begin{equation}
    \textrm{Counting events: } \abs{X} = 2^n
  \end{equation}
\item Binomial Coefficient
  \begin{equation}
    (x+y)^n = \sum_{k=0}^n\binom{n}{k}x^{n-k}y^k
  \end{equation}
\end{enumerate}

\textbf{Probability Function:}
\begin{enumerate}
\item $\mathbb{P}(A):Event\rightarrow [0,1]$
\item $\mathbb{P}(\emptyset) = 0, \mathbb{P}(\Omega)=1$, where
  $\emptyset$ is the empty set, and $\Omega$ is the problem space
\item $\mathbb{P}(A\cap B) = \mathbb{P}(B)\mathbb{P}(A | B)$,
  Condition probability
\item $\mathbb{P}(A\cup B) = \mathbb{P}(A) + \mathbb{P}(B) -
  \mathbb{P}(A\cap B)$
\item $\mathbb{P}(A^c) = 1-\mathbb{P}(A)$, Reason: $A\cap A^c =
  \emptyset \wedge \mathbb{P}(A\cup A^c) = \mathbb{P}(\Omega)
  \Rightarrow \mathbb{P}(A)+\mathbb{P}(A^c) = 1$
\item Given: $A$ has mutually disjoint, $\mathbb{P}(\cup_{i=0}^n
  A_i) = \sum_{i=0}^n\mathbb{P}(A_i) $
\item $\mathbb{P}(A)=\sum_{i=0}^n\mathbb{P}({t_i})$, assuming ${t_i}\cap {t_j} = \emptyset$
\item $A\subseteq B \rightarrow \mathbb{P}(A)\leq \mathbb{P}(B)$
\item Independent iff: $\mathbb{P}(A\cap B) =
  \mathbb{P}(B)\mathbb{P}(B)$, alternatively written
  $\mathbb{P}(A|B) = \mathbb{P}(A)$ for $\mathbb{P}(B) \neq 0$ 
\end{enumerate}

\newpage
\textbf{Random Variables:}
\begin{enumerate}
\item Def:
  \begin{equation}
    \textrm{Random Variable($X$): } X:\Omega\rightarrow \mathbb{R}
  \end{equation}
\item
  \begin{equation}
    \textrm{Prob mass function: } f(x): \mathbb{P}(X=a):X\rightarrow [0,1]
  \end{equation}
\item 
  \begin{equation}
    \textrm{Prob density function: } f(x):
  \end{equation}
\item Cumdist Properties (continuous):
  \begin{enumerate}
  \item Def:
    \begin{equation}
      \textrm{(Countable): }F(x) \equiv \mathbb{P}(X\leq a) =
      \sum_{i=0}^a\mathbb{P}(X_i), a\in\Omega
    \end{equation}
    \begin{equation}
      \textrm{(Continuous): } F(x) \equiv \mathbb{P}(X\leq a) =
      \int_{-\infty}^a\mathbb{P}(X_i)dx
    \end{equation}
  \item $0 \leq F(x) \leq 1$
  \item $ \lim_{x\rightarrow -\infty}F(x) = 0 = \mathbb{P}(\emptyset) \wedge
    \lim_{x\rightarrow \infty}F(x) = 1 = \mathbb{P}(\Omega)$
  \item $\forall (x,y) x<y \rightarrow F(x) \leq F(y)$
  \end{enumerate}
\item Cumdist/Prob Density
  \begin{equation}
    \textrm{Cumdist/Prob density: } F(x) \equiv \int_{-\infty}^{\infty} f(x)dx  
  \end{equation}
  \begin{equation}
    \textrm{Prob density/Cumdist: } f(x) \equiv \frac{d}{dx}F(x)
  \end{equation}
\item Expected($\mu$)
  \begin{equation}
    \textrm{(Countable): } \mathbb{E}(X) \equiv \sum_{i=0}^nX_i\mathbb{P}(X_i)
  \end{equation}
  \begin{equation}
    \textrm{(Continuous): } \mathbb{E}(X) \equiv \int_{-\infty}^{\infty}X_i\mathbb{P}(X_i)dx
  \end{equation}
  \newpage
  \textbf{Moments:}
  \begin{enumerate}
  \item Definition 
    \begin{equation}
      nth\textrm{ Moment: }\mu_n \equiv \mathbb{E}[(X-b)^n]
    \end{equation}
    \begin{align*}
      n = 0& & \textrm{Constant}\\
      n = 1& & \textrm{Expected Value (center)}\\
      n = 2& & \textrm{Measure of Dispersion}\\
      n = 3& & \textrm{Measure of asymmetry}\\
      n = 4& & \textrm{Measure of peakedness}
    \end{align*}
  \item Types of Moments
    \begin{equation}
      \textrm{Raw Moments: } \mu_n' \equiv \mathbb{E}[(x-0)^n]
    \end{equation}
    \begin{equation}
      \textrm{Central Moments: } \mu_n \equiv \mathbb{E}[(x-\mu)^n]
    \end{equation}
  \item Getting Moments
    \begin{enumerate}
    \item Use def of moment (i.e.) $mu_n \equiv \mathbb{E}[(X-b)^n$
    \item $[PGF]$ Probability Generating Function (Descrete only)
    \item $[MGF]$ Moment Generating Function (May not exist)
      \begin{enumerate}
      \item Def
        \begin{equation}
          M_x(t) \equiv \mathbb{E}(e^{tx})
        \end{equation}
      \item Raw moment from $MGF$
        \begin{equation}
          \frac{\partial^n}{\partial t^n} M_x(t)|_{t=0} = \mathbb{E}(X^n)
        \end{equation}
      \end{enumerate}
    \item Characteristic equation
    \end{enumerate}
  \item Generating Central Moments from Raw Moments
    \begin{equation}
      \mu_n = \Sigma_{j=0}^b\binom{n}{k}(-1)^{n-j}\mu_j'\mu^{n-j}
    \end{equation}
  \item Important Moments
    \begin{enumerate}
    \item Mean$(\mu) \equiv \mu_1' = \mathbb{E}(X)$
    \item Variance$(\sigma^2) \equiv \mu_2 = \mathbb{E}((X-\mu)^2)=
      \mathbb{E}(X^2) - \mu^2$
    \item Skew$(\gamma_2) \equiv \frac{\mu_3}{\sigma^3} = \frac{\mathbb{E}((X-\mu)^3)}{\sigma^3}$
    \item Kurtosis$(\gamma_3) \equiv \frac{\mu_4}{\sigma^4} -3$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}
\newpage
\textbf{Joint Probability:}
\begin{enumerate}
\item $f(x,y)$ is the joint probability density function of the sets
  $X$ and $Y$
\item Marginal Density: $f(x) = \int_{-\infty}^{\infty}f(x,y)dy$
\item $X$ and $Y$ are Independent iff $f(x,y) = f(x)f(y)$
\item $F(x,y) = \int_{-\infty}^{x}\int_{-\infty}^{y}f(x,y)dxdy$
\item $f(x,y) = \frac{\partial^2 F}{\partial x \partial y}$
\item
  \begin{equation}
    \textrm{Correlation}(r) =
    \frac{\mathbb{E}[(X-\mu_x)(Y-\mu_y)]}{\sigma_x \sigma_y}
  \end{equation}
\end{enumerate}

\textbf{Limit Theorems:}
\begin{enumerate}
\item Central Limit Theorem
  \begin{enumerate}
  \item Assumptions
    \begin{enumerate}
    \item All $x \in X$ are independent for a random sample
    \item All $X$ have the same distribution, $\mu$ and $\sigma$
    \end{enumerate}
  \item CLT approximated moments
    \begin{enumerate}
    \item Sum of Averages = $n\mu$
    \item Sum of Variances = $n\sigma^2$
    \item $\bar{\sigma}^2 = \frac{\sigma^2}{n}$
    \item $\mathbb{E}(\bar{X}) = \mu$
    \end{enumerate}

  \item Normal Approximation to binomial
    \begin{align*}
      \mathbb{P}(a \leq \mathbb{S} \leq b) \approx
      \mathbb{P}(A-\frac{1}{2} \leq X \leq b + \frac{1}{2}),\textrm{ } mp > s, m(1-p)>s
    \end{align*}
  \item Worst Case Approximation:
    \begin{align*}
      \mathbb{P}(\abs{\bar{x}-p} \leq \epsilon) \equiv
      \mathbb{P}(\abs{\mathbb{Z}} \leq 2\epsilon\sqrt{n})
    \end{align*}
  \end{enumerate}
\item Markov's inequality: applies to any non-negative random variable
  \begin{equation}
    \mathbb{P}(X \geq a) \leq \frac{\mu}{a} , a >0
  \end{equation}
  \begin{equation}
    \mathbb{P}(X \geq k\mu) \leq \frac{1}{k}
  \end{equation}
\item Chebyshev's inequality: upper bound on dist
  \begin{equation}
    \mathbb{P}(\abs{X-\mu} \geq c) \leq \frac{\sigma^2}{c^2}, c>0
  \end{equation}
  \begin{equation}
    \mathbb{P}(\abs{X-\mu} \geq k\sigma) \leq \frac{1}{k^2}
  \end{equation}
\item Law of large numbers
  \begin{equation}
    s
  \end{equation}
\end{enumerate}

\newpage
\textbf{Common Random Variables:}
\begin{enumerate}
\item Bernoulli trials: Independent repeated trials of an experiment with two outcomes only
\item Poisson Process: a stochastic process in which events occur continuously and independently of one another 
\end{enumerate}


\textbf{Distributions:}
\begin{enumerate}
\item $S = $ number of successes, $S: \mathbb{N} \rightarrow \mathbb{N}$
\item $W=$ waiting time, $W:\mathbb{N} \rightarrow \mathbb{N}$
\item $\mathbb{L}=$ life time, $\mathbb{L}: \mathbb{R} \rightarrow
  \mathbb{R}$
\item $p=$ prob of success
\item Binomial Dist. (countable)
  \begin{align*}
    &\abs{\Omega} = 2^n \\
    &\mathbb{P}(S = k) = \binom{n}{k} p^k(1-p)^{m-k} \\
    &\mu = np \\
    &\sigma^2 = np(1-p)
  \end{align*}
\item K-nomial(multinomial) Dist. (countable)
  \begin{align*}
    &\abs{\Omega} = K^n \\
    &\mathbb{P}(S = k) = \prod_{i=0}^k \binom{n-m_i}{m_i} p_i^{m_i} \\
  \end{align*}
\item Negative Binomial Dist. (countable)
  \begin{align*}
    &r=\textrm{number of failures until the experiment is stopped} \\ \\
    &f(x) = \binom{k+r-1}{k}(1-p)^rp^k \\
    &mgf = \frac{1-p}{1-p e^t}^r
  \end{align*}
\item Geometric Dist. (countable)
  \begin{align*}
    &\mathbb{P}(W=k)=p(1-p)^{(k-1)} \\
    &\mathbb{P}(W\leq K) = 1-(1-p)^k \\
    &\mu = \frac{1}{p} \\
    &\sigma^2= \frac{1-p}{p^2}
  \end{align*}
\item Poisson Dist. (countable)
  \begin{align*}
    &\Lambda = t\lambda, \lambda = \textrm{ Longterm average rate} \\
    &\mathbb{P}(S=k) = e^{-\Lambda}\frac{\Lambda^k}{k!} \\
    &\mu = \Lambda \\
    &\sigma^2 = \Lambda
  \end{align*}
\item Uniform Dist (continuous):\\
  \begin{equation}
    p = \frac{1}{b-a}
  \end{equation}
  \begin{displaymath}
    f(x) = \left\{
      \begin{array}{lr}
        p & : x \in [a,b]\\
        0 & : x \notin [a,b]
      \end{array}
    \right.\\
  \end{displaymath}\\
\item Linear Dist (continuous):
  \begin{equation}
    C(b-a)+D(b-a)^2=1
  \end{equation}
  \begin{displaymath}
    f(x) = \left\{
      \begin{array}{lr}
        C+Dx & : x \in [a,b]\\
        0 & : x \notin [a,b]
      \end{array}
    \right.\\
  \end{displaymath}\\
\item Exponential Dist. (continuous)
  \begin{align*}
    &\mathbb{P}(\mathbb{L} > t) = \mathbb{P}(\mathbb{S} = 0) = \frac{\Lambda^0}{0!}e^{-\Lambda}, t>0\\
    &F(t) = \mathbb{P}(\mathbb{L} \leq t) = 1 -\mathbb{P}(\mathbb{L} > t) = 1-e^{-\Lambda}\\ \\
    &f(t) = \frac{d}{dt}F(t) = 
    f(x) = \left\{
      \begin{array}{lr}
        \Lambda e^{-\Lambda} & : x \geq 0\\
        0 & : x < 0
      \end{array}
    \right.\\
    &\mu = \int_{-\infty}^{\infty}f(x)dx = \int_{0}^{\infty}t\lambda
    e^{-\lambda x}dx = \frac{1}{\lambda}\\
    &\sigma^2 = \frac{1}{\lambda^2}\\
    &\textrm{Non-Poisson trials (aging): } \mathbb{P}(\mathbb{L}>t^* |
    \mathbb{L} >t_0) = e^{-\int_{t_0}^{t^*} \lambda(x)dx}
  \end{align*}
\item Standard Normal Dist. (continuous)
  \begin{align*}
    &\phi = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} \\
    &\Phi(\mathbb{Z}) = \mathbb{P}(\mathbb{Z} \leq z) = \int_{-\infty}^z\phi(x)dx = \int_{-\infty}^z\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}} dx\\
    &\mu = 0, \sigma^2 = 0\\
    & X = \sigma\mathbb{Z} + \mu \Leftrightarrow \mathbb{Z} = \frac{X-\mu}{\sigma}
  \end{align*}
\end{enumerate}

\end{document}
