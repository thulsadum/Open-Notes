\documentclass[12pt]{article}
\textwidth = 6.7 in
\textheight = 9.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.2 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

% ***********************************************************
% *********************** HEADER  ***************************
% ***********************************************************

\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}	% Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage{multicol} % Allows for multiple columns
\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
% Sets margins and page size
\pagestyle{empty} % Removes page numbers
\makeatletter % Need for anything that contains an @ command 
\renewcommand{\maketitle} % Redefine maketitle to conserve space
{ \begingroup \vskip 10pt \begin{center} \large {\bf \@title}
    \vskip 10pt \large \@author \hskip 20pt \@date \end{center}
  \vskip 10pt \endgroup \setcounter{footnote}{0} }
\makeatother % End of region containing @ commands
\renewcommand{\labelenumi}{(\alph{enumi})} % Use letters for enumerate
% \DeclareMathOperator{\Sample}{Sample}
\let\vaccent=\v % rename builtin command \v{} to \vaccent{}
\renewcommand{\v}[1]{\ensuremath{\mathbf{#1}}} % for vectors
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} 
% for vectors of Greek letters
\newcommand{\uv}[1]{\ensuremath{\mathbf{\hat{#1}}}} % for unit vector
\newcommand{\abs}[1]{\left| #1 \right|} % for absolute value
\newcommand{\avg}[1]{\left< #1 \right>} % for average
\let\underdot=\d % rename builtin command \d{} to \underdot{}
\renewcommand{\d}[2]{\frac{d #1}{d #2}} % for derivatives
\newcommand{\dd}[2]{\frac{d^2 #1}{d #2^2}} % for double derivatives
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} 
% for partial derivatives
\newcommand{\pdd}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
% for double partial derivatives
\newcommand{\pdc}[3]{\left( \frac{\partial #1}{\partial #2}
  \right)_{#3}} % for thermodynamic partial derivatives
\newcommand{\ket}[1]{\left| #1 \right>} % for Dirac bras
\newcommand{\bra}[1]{\left< #1 \right|} % for Dirac kets
\newcommand{\braket}[2]{\left< #1 \vphantom{#2} \right|
  \left. #2 \vphantom{#1} \right>} % for Dirac brackets
\newcommand{\matrixel}[3]{\left< #1 \vphantom{#2#3} \right|
  #2 \left| #3 \vphantom{#1#2} \right>} % for Dirac matrix elements
\newcommand{\grad}[1]{\gv{\nabla} #1} % for gradient
\let\divsymb=\div % rename builtin command \div to \divsymb
\renewcommand{\div}[1]{\gv{\nabla} \cdot #1} % for divergence
\newcommand{\curl}[1]{\gv{\nabla} \times #1} % for curl
\let\baraccent=\= % rename builtin command \= to \baraccent
\renewcommand{\=}[1]{\stackrel{#1}{=}} % for putting numbers above =
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
% \def\rmdefault{bch} % Use Charter for main text font.

% ***********************************************************
% ********************** END HEADER *************************
% ***********************************************************
% =========================================================
\begin{document}

\begin{center}
  {\LARGE
    \textbf{CS 225}\\
  }
  
\end{center}
C++
\begin{enumerate}
\item C syntax
\item Object Oriented
\item Big three
\item Templates
\item Inheritance
\item Generic Programming (Functors)
\end{enumerate}
Linear Data Structures
\begin{enumerate}
\item Array
\item Linked List
\item Linear ADT's (usually linear)
  \begin{enumerate}
  \item Lists
    \begin{enumerate}
    \item Runtimes:
      \begin{align*}
        &\textrm{SLL(Single Linked} & \textrm{Array}\\
        \textrm{Insert/Remove at front:   } & O(1) & O(1)\\
        \textrm{Insert at curr:   } & O(n) & O(n)\\
        \textrm{Remove at curr:   } & O(1)\textrm{ Hack} & O(n)\\
        \textrm{Insert at i:   } & O(n) & O(n) \\
        \textrm{Remove at i:   } & O(n) & O(n) \\
      \end{align*}
    \end{enumerate}
    \item Stacks (LIFO)
      \begin{align*}
        \textrm{Push: } & O(1) \\
        \textrm{Pop: } & O(1) \\
        \textrm{Find: } & O(n)
      \end{align*}
    \item Queues (FIFO)
      \begin{tabular}{r  l  l }
        & SLL & Array\\
        Push(enqueue): & $O(1)$ (from tail)& $O(1)$ \\
        Pop(dequeue) & $O(1)$ (from head)  & $O(1)$\\
        Find & $O(n)$ & $O(n)$
      \end{tabular}
        

    \end{enumerate}
    Trees (asymmteric graphs)
    \begin{enumerate}
    \item Terminology
      \begin{enumerate}
      \item Leaf: Terminal node with not children
      \item Root: Base node in a directed tree
      \end{enumerate}
    \item Traversals
      \begin{enumerate}
      \item All tree traversals are min $O(n)$
      \item Preorder: Parent - Child - Child
      \item Inorder: Child - Parent - Child
      \item Postorder: Child - Child - Parent
      \item Depth-first order:
        In depth-first order, we always attempt to visit the node farthest from the root that we can, but with the caveat that it must be a child of a node we have already visited. Unlike a depth-first search on graphs, there is no need to remember all the nodes we have visited, because a tree cannot contain cycles. Pre-order is a special case of this. See depth-first search for more information.
      \item Breadth-first order:
Contrasting with depth-first order is breadth-first order, which always attempts to visit the node closest to the root that it has not already visited. See breadth-first search for more information. Also called a level-order traversal.
      \end{enumerate}
    \item N-airy Tree
      \begin{enumerate}
      \item Root: is a tree with a root node in which every node has at most two children.
      \item Full: Every non-leaf node has $2$ leafs 
      \item Perfect: is a full binary tree in which all leaves are at the same depth or same level, and in which every parent has two children.
      \item Complete: is a binary tree in which every level, except possibly the last, is completely filled, and all nodes are as far left as possible.
      \item Balanced: is commonly defined as a binary tree in which the height of the two subtrees of every node never differ by more than 1,[3] although in general it is a binary tree where no leaf is much farther away from the root than any other leaf.
      \item BST:
        \begin{tabular}{r l l}
          & Avg & Worst\\
          insert & $O(lg(n))$ & $O(n)$ \\
          remove & $O(lg(n))$ & $O(n)$ \\
          find & $O(lg(n))$ & $O(n)$ \\
          traversal & $O(n)$ & $O(n)$\\
        \end{tabular}

        \begin{enumerate}
        \item The left subtree of a node contains only nodes with keys less than the node's key.
        \item The right subtree of a node contains only nodes with keys greater than the node's key.
        \item Both the left and right subtrees must also be binary search trees.
        \end{enumerate}
      \item AVL:
        \begin{enumerate}
        \item Runtimes: insert = remove = find = $O(lg(n))$
        \item In computer science, an AVL tree is a self-balancing binary search tree, and it was the first such data structure to be invented.[1] In an AVL tree, the heights of the two child subtrees of any node differ by at most one. Lookup, insertion, and deletion all take $O(log n)$ time in both the average and worst cases, where n is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more tree rotations.
        \item Rotations $O(1)$: Look at the way you fingers move in a
          right hand curl
        \item Left Rotation: out the page right hand rule
        \item Right Rotation: into of the page right hand rule
        \end{enumerate}
      \item B-Trees ($m$):
        \begin{enumerate}
        \item Search: $O( log_m(n))$
        \item In computer science, a B-tree is a tree data structure
          that keeps data sorted and allows searches, sequential
          access, insertions, and deletions in logarithmic time. The
          B-tree is a generalization of a binary search tree in that a
          node can have more than two children. (Comer 1979, p. 123)
          Unlike self-balancing binary search trees, the B-tree is
          optimized for systems that read and write large blocks of
          data. It is commonly used in databases and filesystems.
        \item $m-1$ children
        \end{enumerate}
      \item KD-tree
      \item Heap:
        \begin{enumerate}
        \item buildHeap: $O(n)$
        \item sort (already a heap) $O(lg(n))$
        \item Heap Sort: $O(nlg(n))$ worst
        \end{enumerate}
      \end{enumerate}
    \end{enumerate}
    Graphs
    \begin{enumerate}
    \item Vocab:
      \begin{enumerate}
      \item Vertex: $v\in V$, Edges: $e\in E$, Graph: $G$, $n=
        \abs{V}, m = \abs{E}$
      \item Incident Edges: $I(v) = {(v,u) \in E}$
      \item Degree: $deg(v) = \abs{I}$
      \item Path: - Sequence of $v$ connected by $e$
      \item Cycle - path with common begin and end $v$
      \item Simple Graph: $G$ with no self loops and no muliedges
      \item Subgraph:
      \item Complete Subgraph:
      \item Connected Subgraph:
      \item Connected comp:
      \item Acyclic subgraph:
      \item Spanning tree: connected acyclic subgraph
      \end{enumerate}

    \item Properties
      \begin{enumerate}\item 
      \item Every connected tree graph $G$ has $\abs{V}-1= n-1$ edges,
        $\abs{V} = n$
      \item $\abs{E} = {x\in \mathbf{N}\cap[n-1,U]}$, for upper bound $U$
      \item Simple Graph $U=\binom{n}{2} \rightarrow U O(n^2)$,
        Non-Simple $U=\infty$
      \end{enumerate}

    \item Graph ADT:
      \begin{enumerate}
      \item insertVertex($key, data$)
      \item insertEdge($v_1,v_2,key,data$)
      \item removeEdge($e$)
      \item areAdjacent($v_1,v_2,$)
      \item orign($e$)
      \item destination($e$)
      \end{enumerate}

    \item Implementations: 
      \begin{tabular}{ r l l l }
        Function & EdgeList & Adj Matrix & Adj List \\
        insertVertex & $O(1)$ & $O(n^2)/O(n)$ & $O(1)$ \\
        removeVertex & $O(m)$ & $O(n)$ & $O(deg(v))$ \\
        areAdjacent & $O(m)$ & $O(1)$ & $O(min[deg(v_1),deg(v_2)])$\\
        incidentEdges & $O(m)$ & $O(n)$ & $O(deg(v)$\\
      \end{tabular}


% from wikipedia
    \item MST: In computer science, a B-tree is a tree data structure that keeps data sorted and allows searches, sequential access, insertions, and deletions in logarithmic time. The B-tree is a generalization of a binary search tree in that a node can have more than two children. (Comer 1979, p. 123) Unlike self-balancing binary search trees, the B-tree is optimized for systems that read and write large blocks of data. It is commonly used in databases and filesystems.
    \item In mathematics, a graph is an abstract representation of a set of objects where some pairs of the objects are connected by links. The interconnected objects are represented by mathematical abstractions called vertices, and the links that connect some pairs of vertices are called edges. Typically, a graph is depicted in diagrammatic form as a set of dots for the vertices, joined by lines or curves for the edges. Graphs are one of the objects of study in discrete mathematics.
    \item Traversals:
      \begin{enumerate}
      \item Existence of back/cross edges denotes a cycle
      \item DST: In-order analog
        \begin{enumerate}
        \item Runtime: $O(n+m)$
        \end{enumerate}

      \item BST: Level Order analog
        % \begin{enumerate}
        % \item Runtime: adjList: $O(n+m)$, adjMatrix: $O(n^2)$
        % \item Always finds min path
        % \item Enqueue the root node.
        % \item Dequeue a node and examine it.
        % \item -----If the element sought is found in this node, quit the search and return a result.
        % \item -----Otherwise enqueue any successors (the direct child nodes) that have not yet been discovered.
        % \item If the queue is empty, every node on the graph has been examined – quit the search and return "not found".
        % \item If the queue is not empty, repeat from Step 2.
        % \end{enumerate}

      \item Kruskal's algorithm:
        \begin{enumerate}
        \item Def: an algorithm in graph theory that finds a minimum spanning tree for a connected weighted graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component). Kruskal's algorithm is an example of a greedy algorithm.
        \item Runtime: $O(mlog(n)+m)$
        \end{enumerate}

      \item Prim's algorithm:
        \begin{enumerate}
        \item Def: a greedy algorithm that finds a minimum spanning
          tree for a connected weighted undirected graph. This means
          it finds a subset of the edges that forms a tree that
          includes every vertex, where the total weight of all the
          edges in the tree is minimized. The algorithm was developed
          in 1930 by Czech mathematician Vojtěch Jarník and later
          independently by computer scientist Robert C. Prim in 1957
          and rediscovered by Edsger Dijkstra in 1959. Therefore it is
          also sometimes called the DJP algorithm, the Jarník
          algorithm, or the Prim–Jarník algorithm.
        \item Runtime:
          \begin{tabular*}{1.0\linewidth}{r l l}
                  & adjMatrix & adjList\\
             heap & $O(n+lg(n)+n^2+mlg(n))$ & $O(n+nlg(n)+mlg(n)$\\
             unsorted Array & $O(n^2)$ & $O(n^2)$
          \end{tabular*}

        \end{enumerate}

      \item Dijkstra’s Algorithm:
        \begin{enumerate}
        \item Def: conceived by Dutch computer scientist Edsger Dijkstra in 1956 and published in 1959, is a graph search algorithm that solves the single-source shortest path problem for a graph with nonnegative edge path costs, producing a shortest path tree. This algorithm is often used in routing and as a subroutine in other graph algorithms.
        \item Runtime:
          \begin{tabular*}{1.0\linewidth}{r l l}
                  & adjMatrix & adjList\\
             heap & $O(lg(n)+n^2+mlg(n))$ & $O(nlg(n)+mlg(n)$\\
             unsorted Array & $O(n^2)$ & $O(n^2)$
          \end{tabular*}
        \end{enumerate}

      \end{enumerate}
    \end{enumerate}
    Other ADTS
    \begin{enumerate}
    \item Map(Associative Array)(Hash Maps)
      \begin{enumerate}
      \item SUHA: Simple Uniform Hashing Assumption
      \item Seperate Chaining:
        \begin{tabular}{r l l}
          & Worst Case & SUHA\\
          Insert & $O(1)$ & $O(1)$\\
          Remove Sucessful & $O(n)$ & $O(1/2(n/N))\approx{O(1)}$\\
          Remove Unsucessful & $O(n)$ & $O(n/N)\approx{O(1)}$
        \end{tabular}
      \item Linear Probing:
      \item Double Hashing:
      \end{enumerate}

    \item Priority Queues: Either Heap or Unsorted Array, Always give
      the min or max of the set
    \item DistJoint Sets (Up-Trees)
      \begin{tabular}{r l}
        Union & $O(1)$\\
        Find & $O(lg(n))$
      \end{tabular}

    \end{enumerate}
  \end{enumerate}
  \end{document}