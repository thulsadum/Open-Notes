\documentclass[12pt]{article}
\textwidth = 6.7 in
\textheight = 9.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.2 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

% ***********************************************************
% *********************** HEADER  ***************************
% ***********************************************************

\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}	% Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage{multicol} % Allows for multiple columns
\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
% Sets margins and page size
\pagestyle{empty} % Removes page numbers
\makeatletter % Need for anything that contains an @ command 
\renewcommand{\maketitle} % Redefine maketitle to conserve space
{ \begingroup \vskip 10pt \begin{center} \large {\bf \@title}
    p	\vskip 10pt \large \@author \hskip 20pt \@date \end{center}
  \vskip 10pt \endgroup \setcounter{footnote}{0} }
\makeatother % End of region containing @ commands
\renewcommand{\labelenumi}{(\alph{enumi})} % Use letters for enumerate
% \DeclareMathOperator{\Sample}{Sample}
\let\vaccent=\v % rename builtin command \v{} to \vaccent{}
\renewcommand{\v}[1]{\ensuremath{\mathbf{#1}}} % for vectors
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} 
% for vectors of Greek letters
\newcommand{\uv}[1]{\ensuremath{\mathbf{\hat{#1}}}} % for unit vector
\newcommand{\abs}[1]{\left| #1 \right|} % for absolute value
\newcommand{\avg}[1]{\left< #1 \right>} % for average
\let\underdot=\d % rename builtin command \d{} to \underdot{}
\renewcommand{\d}[2]{\frac{d #1}{d #2}} % for derivatives
\newcommand{\dd}[2]{\frac{d^2 #1}{d #2^2}} % for double derivatives
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} 
% for partial derivatives
\newcommand{\pdd}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
% for double partial derivatives
\newcommand{\pdc}[3]{\left( \frac{\partial #1}{\partial #2}
  \right)_{#3}} % for thermodynamic partial derivatives
\newcommand{\ket}[1]{\left| #1 \right>} % for Dirac bras
\newcommand{\bra}[1]{\left< #1 \right|} % for Dirac kets
\newcommand{\braket}[2]{\left< #1 \vphantom{#2} \right|
  \left. #2 \vphantom{#1} \right>} % for Dirac brackets
\newcommand{\matrixel}[3]{\left< #1 \vphantom{#2#3} \right|
  #2 \left| #3 \vphantom{#1#2} \right>} % for Dirac matrix elements
\newcommand{\grad}[1]{\gv{\nabla} #1} % for gradient
\let\divsymb=\div % rename builtin command \div to \divsymb
\renewcommand{\div}[1]{\gv{\nabla} \cdot #1} % for divergence
\newcommand{\curl}[1]{\gv{\nabla} \times #1} % for curl
\let\baraccent=\= % rename builtin command \= to \baraccent
\renewcommand{\=}[1]{\stackrel{#1}{=}} % for putting numbers above =
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
% \def\rmdefault{bch} % Use Charter for main text font.

% ***********************************************************
% ********************** END HEADER *************************
% ***********************************************************
% =========================================================
\begin{document}

\begin{center}
  {\LARGE
    \textbf{CS232 Review}\\
    \normalsize
  }
\end{center}
  \textbf{MidTerm 1}  
  \begin{enumerate}
  \item Bitwise Logical Programming (shift $<<$ $>>$ ,and $\&$, or
    $|$,not $~$,xor $^\wedge$)
  \item MIPS assembly
    \begin{enumerate}
    \item arithmetic, memory, control flow
    \item MIPS $<->$ C
    \end{enumerate}

  \item Functions:
    \begin{enumerate}
    \item Register saving convetions
      \begin{enumerate}
      \item $a$ registers are caller saved
      \item $s$ registers are calle saved
      \end{enumerate}

    \item stack (and its use for saving state of program)
      \begin{enumerate}
      \item kept track of via $sp$ register
      \item Grows down
      \end{enumerate}

    \end{enumerate}
  \item Basic understanding of assembly level concepts
    \begin{enumerate}
    \item pointers, pointer arithmetic, structures
    \item machine language
    \item variable types in low level languages (not explicit, its how
      its used)
    \item compiling, linking, loading, MIPS memory image
    \item $\frac{90}{10}$ rule : spend $90\%$ of your time in $10\%$ of your code
    \end{enumerate}

  \end{enumerate}
  \textbf{MidTerm 2}
  \begin{enumerate}
  \item Generic datapath concepts
  \item Single Cycle Datapath: 1 instruction per cycle...always (set
    clock cycle to the longest instruction time)
  \item Pipeling and the Pipedlined Datapath
    \begin{enumerate}
    \item Ideal execution time = time to fill + 1 cycle per instruction
      \begin{enumerate}
      \item Not achieved due to hazards (and slow memory access)
      \end{enumerate}

    \item Dependences: A value being modified and used in later instructions
    \item Data Hazards: Occurs when dependent instructions exectuted
      before data has finished the writeback state.
    \item Forwarding: Using pipeline registers' values instead of
      register file. Can be done only if correct data exists in the
      pipeline when it is needed (ld -> ld doesn't work because EX
      requires at the begining but Mem has the good value at the end)
    \item Control hazards: Possible incorrect PC value due to branches
      and jumps
    \item Branch resolution: If branch is incorretly predicted, bad
      data must be 'flushed' out of the pipeline by turning the
      control signals into retroactive nops.
    \item Branch Prediction: Act of predicting a whether a branch is
      taken. (Can be based on previous behaviour)
    \item Stalling: Repeating the same stage of the pipeline and not
      incrementing the PC counter.
    \end{enumerate}
  \item Performance
    \begin{enumerate}
    \item Dynamic Instruction Count: instructions actually executed
    \item Cycles Per Instruction(CPI): Average cycles per instruction
    \item Clock Cycle Time: Set to the max time it requires a pipeline
      stage to complete. $\frac{1}{cpuFreq}$
    \item Computing speedup
      \begin{enumerate}
      \item $cpuTime = instructions\cdot CPI \cdot cycleTime$
      \item $speedUp = \frac{performance_0}{performance_f}$
      \end{enumerate}

    \item Throughput vs. Latency
      \begin{enumerate}
      \item Throughput: tasks per unit time, i.e. bandwidth
      \item Latency: unit time per task, i.e. response time
      \end{enumerate}

    \item Amdahl's Law
      \begin{enumerate}
      \item States that the perfomrance enhancement possible with a given improvememt is limited by the amount that the feature to be improved affects execution time.
      \item Most importantly, Amdahl's law states that if one
        parellisizes away some work, one is limited by the seriel component.
      \item $Time_{total} = \frac{Time_{old}} { Time_{improved}} + Time_{unaffected}$
      \end{enumerate}

    \end{enumerate}
  \item I/O programming and interrupts
    \begin{enumerate}
    \item Memory-mapped I/O
      \begin{enumerate}
      \item Address space divided, with some representing physical memory locations and some referencing peripherals
      \item CPU writes to appropriate I/O address which is trasmited along the bus so that devices can know when they are the target
      \end{enumerate}

    \item Structure of the interrupt/exception Handler
      \begin{enumerate}
      \item Interrupts/exceptions must be enabled
      \item Execution is stopped, control given to OS, the OS executes
        interrupt Handler
      \item Interrupt/exception Handler
        \begin{enumerate}
        \item Allocated some kernel memory (can't use stack because
          that may have caused the interrupt/exception)
        \item Saves assembler temporaries
        \item Backs up registers it will work with
        \item Read cause register
        \item Handle interrupt/exception (and acknowledge interrupt/exception)
        \item Restore system back to pre interrupt/exception state
        \item Jump back into code. 
        \end{enumerate}
      \item Endianness: Should bytes be stored most sig to least sig
        or the othe way around?
        \begin{enumerate}
        \item Little vs Big
        \item Swizzling: the act of converting endianness for inter
          machine communication. (internet done in Big Endian)
        \end{enumerate}

      \end{enumerate}

    \end{enumerate}
  \end{enumerate}
  \textbf{MidTerm 3}
  \begin{enumerate}

  \item Caches
    \begin{enumerate}
    \item Locality
      \begin{enumerate}
      \item Spacial Locality: If a program accesses one memory address, there is a good chance that it will access other nearby addresses.
      \item Temporal Locality: If a program accesses one memory address, there is a good chance that it will access the same address again.
      \end{enumerate}

    \item Associatity: Relates how addresses get mapped inside of
      the cache to the number of sets n.
      \begin{enumerate}
      \item Set Associative: cache is divided into groups of blocks called sets. Each address maps to one set in which it can be placed anywhere.
      \item Direct Mapped: Set size of 1, index points to a single block chunk. Each memory address belongs in exactly one block.
      \item Fully Associative: Set size of n, data can be stored in any cache block. Least Recently Used replaced when cache is full. The entire tag must be stored and every tag in the cache must be checked for cache hits/misses
      \end{enumerate}
      
      \item LRU: least recently used data: Replacing elements in a cache that havenâ€™t been used in a while. Helps keep temporal locality. On miss, the LRU is replaced. On hit, the LRU is updated.*often used or approximated
        because of its expensive.

      \item Multilevel caches: A cache that utilized 2 or more separate
      caches to store information. Typically the lower caches are
      small/fast/expensive where as the higher caches are
      large/fast/cheap.\\
      Note: If one can ensure almost certain hits in the higher caches
      and relatively good rates in the lower caches this greatly
      increases speed.

    \item Write-back: Writing is done only to the cache. A modified
      cache block is ``written'' back into higher memory just before
      it is stored. (requires a dirty bit to denote that it has been
      written to)

    \item Write-Through: On write hit, updates both the cache and the
      main memory  when writing.

    \item Allocate on write: On write miss, load into the cache then write to cache 

    \item Computing block offset, index, tag
      \begin{enumerate}
      \item N-Way associative cache has N blocks per set. Thus
        address $\rightarrow$ cache address is given by (= denotes
        number of bits in the bitstring at that point dedicated.
        \begin{equation}
          |Tag=m-s-o|Index=s|offset=o|, m = addressSize
        \end{equation}
        \begin{equation}
          |t = m-o-s | s = lg(\frac{size}{blockSize\cdot N})|o = lg(\frac{blockSize}{byte})|
        \end{equation}
      \end{enumerate}

    \item Computing the size of cache and number of bits to implement it
      \begin{enumerate}
      \item $cacheSize =  blocks \cdot sizeOfBlock = cacheSize$
      \item $sizeOfBlock =  (data + tag + valid + dirty + lruBits)$
      \end{enumerate}

    \item Calculating cache performance and Average Memory Acess Time (AMAT) calculations.
      \begin{enumerate}
      \item :
        \begin{equation}
          \label{eq: AMAT}
          AMAT = HitTime + (MissRate * Miss) 
        \end{equation}
        \begin{equation}
          MemoryStallCycles = MemoryAccesses * MissRate * Miss penalty
        \end{equation}
      \end{enumerate}
    \item Memory access pattern and linearizing array access: 
    \item Memory access pattern analysis: Look for temperal and
      spacial locaility. Calculate mis-rates and how to maximize
      locality. (Blocking/ Tiling)
    \end{enumerate}
  \item Cache-Aware programming
    \begin{enumerate}
    \item Rearranging loops to improve spatial locality
      \begin{enumerate}
      \item stepping through columns in a row exploits spatial locality
        misses once per block chunk
      \item stepping through rows in one column has no spatial locality
        misses $100\%$ of the time
      \end{enumerate}

    \item Using blocking to improve temporal locality
      \begin{enumerate}
      \item used when cache is exceeded and overwritten often, resulting in very few useful values in cache
      \item implemented by striding loops and adding smaller inner loops
      \end{enumerate}

    \item When to software pre-fetch
      \begin{enumerate}
      \item Non-Stride access patterns
      \item Linked Data Structures.
      \end{enumerate}

    \end{enumerate}

  \item Virtual Memory (VM):
    \begin{enumerate}
    \item Indirection: Adding an intermediate interface that controls endpoints
    \item Use of indirection in VM: Virtual Memory $\rightarrow$ OS
      $\rightarrow$ physical memory.This eliminates the need for
      explicit overlaying in programs.
    \item Advantages of VM.
      \begin{enumerate}
      \item Larger than memory processes: We can store ``memory'' on
        disk via paging. Also because its virtual we can say we have
        ``n'' bits of mem but only have to allocate as much as needed.
      \item Identical/Conflicting address mapping: 2 programs can both
        map to the same address because the OS/PageTable deals with
        actual mapping. i.e. sure you can have $0x4004$ (hands you $0xff0f00$)
      \item Sand-boxing processes: The page table has permission bits
        that tells the OS who can touch the cookie jar.
      \item Controlled sharing between processes: If two addresses
        need to share, it can be done under adult(OS) super vision
        because of the page table's permission bits.
      \end{enumerate}
    \item Page table: Look up table that maps virtual to physical memory
      \begin{enumerate}
      \item page table register points to the page table.
      \item page table is indexed by the VPN
      \item page table contains PPN, valid bit, and dirty bit
      \end{enumerate}

    \item Translation look-aside buffer (TLB): Looking up the page
      table translations sucks....so we cache it...aka TLB
      \begin{enumerate}
      \item uses VPN as tag to PPNs
      \end{enumerate}

    \item Hierarchical page table: Page Directories $\rightarrow$ Page
      Entry $\rightarrow$ Page Table $\rightarrow$ Address . (note
      saves memory because not all Entries and Directories are
      filled.
      \begin{enumerate}
      \item  page tables should be page sizes so they themselves can be
        looked up easily....
      \end{enumerate}

    \item Page faults
      \begin{enumerate}
      \item page is not in memory and must be pulled from disk
      \item request page to replace (uses approximate LRU and write-back)
      \item Pages are large to minimize miss rate
      \item Pages are fully associative to minimize miss rate
      \end{enumerate}

    \end{enumerate}

  \item Hard Disks
    \begin{enumerate}
    \item Platter: Cyclinder that houses magnetic particles organized
      in tracks:
    \item Head: Reads/Writes magnetics particles (moves by arm)
    \item Track: Circular collection of sectors
    \item Sector: Partition of Track where data is actually stored
    \item 
      \begin{equation}
        readWriteTime = Seek+RotDely +sectorRead\cdot sectors
      \end{equation}

    \item Random vs Sequential read/write performance
    \item Solid state disks (SSD): Flash memory based, everything but
      random writes it very fast. (writes are slow due to having to
      clear the whole block before to write)
    \end{enumerate}

    \begin{enumerate}
    \item Error Correcting Codes
      \begin{enumerate}
      \item Error detection vs. Error correction:
        \begin{enumerate}
        \item Error Detection is the ability to detect that somehow a
          bit (or more) has been flipped.
        \item Error Correction is the ability to detect an error has
          occurred and guess what it might have been before being flipped.
        \item Relationship: Error Detection is achievable with fewer
          bits and can be done to a further level of error (more bits
          flipped) but does not correct. Correction gives up the
          ability to detect as far but adds the ability to guess
          initial states.
        \end{enumerate}
      \item Parity: Extra bit that is the result of $xor$ the bitstring
        (odd or even). Parity is recomputed when reading Ram and
        checked. If $P_{stored} \neq P_{computed}$ an error has occured.
      \item Hamming distance: Minimum path of states to no longer
        detect/correct errors.
      \item SECDED: Single-bit Error Correction, Double-bit Error
        Detection. Standard in ECC protect SDRAM.
        \begin{enumerate}
        \item Hamming Dist $ = 4$
        \end{enumerate}

      \end{enumerate}

    \item Writing code to analyze cache with a hardware pre-fetcher
      \begin{enumerate}
      \item Needs to loop such that the stride can't be predicted by
        prefecter (linked structures or uniform random
        variables are good canidates)
      \end{enumerate}

    \end{enumerate}

  \end{enumerate}
  \textbf{Post-Exam 3}
  \begin{enumerate}
  \item Multi-core
    \begin{enumerate}
    \item Definition: A group of processors with a shared memory
      system. Used because higher clock speed is very power
      hungry/impractical and better exposes parallelism. 
    \item Thread: A scheduled unit of processing. Can interact with
      other threads or remain isolated.
    \item Cache Coherence (MSI model):
      \begin{enumerate}
      \item Modified: Only one copy allowed. Can be changed.
      \item Shared: Many copies allowed. Cannot be changed.
      \item Invalid: No copies (except in main memory).
      \item GetS: Requests a shared version of memory. Demotes
        Modified states to Shared.
      \item GetX: Requests an exclusive(modifiable) version of
        memory. All other versions Invalidated.
      \item Upgrade: Promote Shared state to Modified. Invalidates all
        other versions.
      \item source data: if demotion from modified to shared, occurs a
        processor can source the data to another processor instead of
        getting it from main memory (which is slower.)
      \item write back: if no processor will have a copy of the
        modified data then it must be written back to main memory.
      \item Share: Accessing the same data in multiple processors
        requiring numerous state changes (slow)
      \item False sharing: Accessing data in the same cache block
        effectively forcing the processors to fight over exclusive
        state. *avoided by moving data to another cache block (padding
        or some other means)
      \end{enumerate}

    \item Sequential Consistency: Strongest Memory Access model. All
      instructions are sequentially accessed within their own thread
      and thus are mearly interleaved with other threads if they exist.(Slow)
    \item Locking/Blocking/Fencing: Because sequential consistency
      isn't guarenteed (or even the default on most systems) one can
      force critical sections of code into sequential consistency via
      'fencing' off the section. This is supported in hardware via the
      Compare-and-swap (CAS) call. Locking/Block is a common
      implementation of this.
    \end{enumerate}

  \item Exploiting Parallelism
    \begin{enumerate}
    \item Vector Processing: Taking a single register and treating it
      as multiple piece of data so that arithmetic/logic can be performed on
      each section simulatinously. Single Instuction Multiple Data
      (SIMD) is an example implentation given by intel.
    \item Speed Ups via vector processing: limited by $\frac{register
      Size}{dataSize}$ i.e. how many independent elements can be done
      in one instruction and the cost of syncing them (ideally none)
    \item Fork-Join parallelism model: Have a master thread and during
      parallelisable sections of code, 'fork' of independent threads
      and then sync when done to the master thread.
    \item Speed Ups via multi-core parellism: Ignoring cache speed
      ups (may result in super linear growth), one expects linear
      growth with the number of processors minus the overhead of
      splitting/syncing. Again limited by Amdahl's Law.
    \end{enumerate}

  \end{enumerate}
\end{document}
% LocalWords:  LRU Associatity associatity
