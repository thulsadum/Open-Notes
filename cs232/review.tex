\documentclass[12pt]{article}
\textwidth = 6.7 in
\textheight = 9.2 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.2 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2in
\parindent = 0.0in

% ***********************************************************
% *********************** HEADER  ***************************
% ***********************************************************

\usepackage{amsmath} % AMS Math Package
\usepackage{amsthm} % Theorem Formatting
\usepackage{amssymb}	% Math symbols such as \mathbb
\usepackage{graphicx} % Allows for eps images
\usepackage{multicol} % Allows for multiple columns
\usepackage[dvips,letterpaper,margin=0.75in,bottom=0.5in]{geometry}
% Sets margins and page size
\pagestyle{empty} % Removes page numbers
\makeatletter % Need for anything that contains an @ command 
\renewcommand{\maketitle} % Redefine maketitle to conserve space
{ \begingroup \vskip 10pt \begin{center} \large {\bf \@title}
    p	\vskip 10pt \large \@author \hskip 20pt \@date \end{center}
  \vskip 10pt \endgroup \setcounter{footnote}{0} }
\makeatother % End of region containing @ commands
\renewcommand{\labelenumi}{(\alph{enumi})} % Use letters for enumerate
% \DeclareMathOperator{\Sample}{Sample}
\let\vaccent=\v % rename builtin command \v{} to \vaccent{}
\renewcommand{\v}[1]{\ensuremath{\mathbf{#1}}} % for vectors
\newcommand{\gv}[1]{\ensuremath{\mbox{\boldmath$ #1 $}}} 
% for vectors of Greek letters
\newcommand{\uv}[1]{\ensuremath{\mathbf{\hat{#1}}}} % for unit vector
\newcommand{\abs}[1]{\left| #1 \right|} % for absolute value
\newcommand{\avg}[1]{\left< #1 \right>} % for average
\let\underdot=\d % rename builtin command \d{} to \underdot{}
\renewcommand{\d}[2]{\frac{d #1}{d #2}} % for derivatives
\newcommand{\dd}[2]{\frac{d^2 #1}{d #2^2}} % for double derivatives
\newcommand{\pd}[2]{\frac{\partial #1}{\partial #2}} 
% for partial derivatives
\newcommand{\pdd}[2]{\frac{\partial^2 #1}{\partial #2^2}} 
% for double partial derivatives
\newcommand{\pdc}[3]{\left( \frac{\partial #1}{\partial #2}
  \right)_{#3}} % for thermodynamic partial derivatives
\newcommand{\ket}[1]{\left| #1 \right>} % for Dirac bras
\newcommand{\bra}[1]{\left< #1 \right|} % for Dirac kets
\newcommand{\braket}[2]{\left< #1 \vphantom{#2} \right|
  \left. #2 \vphantom{#1} \right>} % for Dirac brackets
\newcommand{\matrixel}[3]{\left< #1 \vphantom{#2#3} \right|
  #2 \left| #3 \vphantom{#1#2} \right>} % for Dirac matrix elements
\newcommand{\grad}[1]{\gv{\nabla} #1} % for gradient
\let\divsymb=\div % rename builtin command \div to \divsymb
\renewcommand{\div}[1]{\gv{\nabla} \cdot #1} % for divergence
\newcommand{\curl}[1]{\gv{\nabla} \times #1} % for curl
\let\baraccent=\= % rename builtin command \= to \baraccent
\renewcommand{\=}[1]{\stackrel{#1}{=}} % for putting numbers above =
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{dfn}{Definition}
\theoremstyle{remark}
\newtheorem*{rmk}{Remark}
% \def\rmdefault{bch} % Use Charter for main text font.

% ***********************************************************
% ********************** END HEADER *************************
% ***********************************************************
% =========================================================
\begin{document}

\begin{center}
  {\LARGE
    \textbf{CS232 Review}\\
    \normalsize
  }
\end{center}
  \textbf{MidTerm 1}  
  \begin{enumerate}
  \item Bitwise Logical Programming (shift $<<$ $>>$ ,and $\&$, or
    $|$,not $~$,xor $^\wedge$)
  \item MIPS assembly
    \begin{enumerate}
    \item arithmetic, memory, control flow
    \item MIPS $<->$ C
    \end{enumerate}
  \item Functions:
    \begin{enumerate}
    \item Register saving convetions
    \item stack (and its use for saving state of program)
    \item recursion
    \end{enumerate}
  \item Basic understanding of assembly level concepts
    \begin{enumerate}
    \item pointers, pointer arithmetic, structures
    \item machine language
    \item variable types in low level languages (not explicit, its how
      its used)
    \item compiling, linking, loading, MIPS memory image
    \item 90/10 rule : spend 90\% of your time in 10\% of your code
    \item Endianness: the convention used in the machine for ordering
      bytes in memory.
    \end{enumerate}
    \begin{enumerate}
    \item I/O programming and interrupts
    \item Performace and Performance Turning
    \item Single-cycle MIPS implementation
    \end{enumerate}
  \end{enumerate}
  \textbf{MidTerm 2}
  \begin{enumerate}
  \item Generic datapath concepts
  \item Single Cycle Datapath: 1 instruction per cycle...always (set
    clock cycle to the longest instruction time)
  \item Pipeling and the Pipedlined Datapath
    \begin{enumerate}
    \item Ideal execution time = time to fill + 1 cycle per instruction
      \begin{enumerate}
      \item Not achieved due to hazards (and slow memory access)
      \end{enumerate}

    \item Dependences: A value being modified and used in later instructions
    \item Data Hazards: Occurs when dependent instructions exectuted
      before data has finished the writeback state.
    \item Forwarding: Using pipeline registers' values instead of
      register file. Can be done only if correct data exists in the
      pipeline when it is needed (ld -> ld doesn't work because EX
      requires at the begining but Mem has the good value at the end)
    \item Control hazards: Possible incorrect PC value due to branches
      and jumps
    \item Branch resolution: If branch is incorretly predicted, bad
      data must be 'flushed' out of the pipeline by turning the
      control signals into retroactive nops.
    \item Branch Prediction: Act of predicting a whether a branch is
      taken. (Can be based on previous behaviour)
    \item Stalling: Repeating the same stage of the pipeline and not
      incrementing the PC counter.
    \end{enumerate}
  \item Performance
    \begin{enumerate}
    \item Dynamic Instruction Count: instructions actually executed
    \item Cycles Per Instruction(CPI): Average cycles per instruction
    \item Clock Cycle Time: Set to the max time it requires a pipeline
      stage to complete. $\frac{1}{cpuFreq}$
    \item Computing speedup
      \begin{enumerate}
      \item $cpuTime = instructions\cdot CPI \cdot cycleTime$
      \item $speedUp = \frac{performance_0}{performance_f}$
      \end{enumerate}

    \item Throughput vs. Latency
      \begin{enumerate}
      \item Throughput: tasks per unit time, i.e. bandwidth
      \item Latency: unit time per task, i.e. response time
      \end{enumerate}

    \item Amdahl's Law
      \begin{enumerate}
      \item States that the perfomrance enhancement possible with a given improvememt is limited by the amount that the feature to be improved affects execution time.
      \item Most importantly, Amdahl's law states that if one
        parellisizes away some work, one is limited by the seriel component.
      \item $Time_{total} = \frac{Time_{old}} { Time_{improved}} + Time_{unaffected}$
      \end{enumerate}

    \end{enumerate}
  \item I/O programming and interrupts
    \begin{enumerate}
    \item Memory-mapped I/O
      \begin{enumerate}
      \item Address space divided, with some representing physical memory locations and some referencing peripherals
      \item CPU writes to appropriate I/O address which is trasmited along the bus so that devices can know when they are the target
      \end{enumerate}

    \item Structure of the interrupt/exception Handler
      \begin{enumerate}
      \item Interrupts/exceptions must be enabled
      \item Execution is stopped, control given to OS, the OS executes
        interrupt Handler
      \item Interrupt/exception Handler
        \begin{enumerate}
        \item Allocated some kernel memory (can't use stack because
          that may have caused the interrupt/exception)
        \item Saves assembler temporaries
        \item Backs up registers it will work with
        \item Read cause register
        \item Handle interrupt/exception (and acknowledge interrupt/exception)
        \item Restore system back to pre interrupt/exception state
        \item Jump back into code. 
        \end{enumerate}
      \item Endianness: Should bytes be stored most sig to least sig
        or the othe way around?
        \begin{enumerate}
        \item Little vs Big
        \item Swizzling: the act of converting endianness for inter
          machine communication. (internet done in Big Endian)
        \end{enumerate}

      \end{enumerate}

    \end{enumerate}
  \end{enumerate}
  \textbf{MidTerm 3}
  \begin{enumerate}

  \item Caches
    \begin{enumerate}
    \item LRU: least recently used data, often used or approximated to
      decide what to kick out of the cache
    \item
      Spacial Locality: Things near what was just used tend to get used
      again.\\
      Temporal Locality: Things that were just used, tend to get reused.
    \item Associatity: Relates how addresses get mapped inside of
      the cache to the number of sets n. (Direct Map equal n sets for
      n addresses, Fully associative 1 set for n addresses)
    \item Write-back: Writing is done only to the cache. A modified
      cache block is ``written'' back into higher memory just before
      it is stored. (requires a dirty bit to denote that it has been
      written to)
    \item Multilevel caches: A cache that utilized 2 or more separate
      caches to store information. Typically the lower caches are
      small/fast/expensive where as the higher caches are
      large/fast/cheap.\\
      Note: If one can ensure almost certain hits in the higher caches
      and relatively good rates in the lower caches this greatly
      increases speed.
    \item Computing cache size, bits, block offset, index, tag, and associatity
      \begin{enumerate}
      \item N-Way associative cache has N blocks per set. Thus
        address $\rightarrow$ cache address is given by (= denotes
        number of bits in the bitstring at that point dedicated.
        \begin{equation}
          |Tag=m-s-o|Index=s|offset=o|, m = addressSize
        \end{equation}
        \begin{equation}
          |t = m-o-s | s = lg(\frac{size}{blockSize\cdot N})|o = lg(\frac{blockSize}{byte})|
        \end{equation}
      \end{enumerate}
    \item Calculating cache performance and Average Memory Acess Time (AMAT) calculations.
      \begin{enumerate}
      \item :
        \begin{equation}
          \label{eq: AMAT}
          AMAT = HitTime + (MissRate * Miss) 
        \end{equation}
        \begin{equation}
          MemoryStallCycles = MemoryAccesses * MissRate * Miss penalty
        \end{equation}
      \end{enumerate}
    \item Memory access pattern and linearizing array access: 
    \item Memory access pattern analysis: Look for temperal and
      spacial locaility. Calculate mis-rates and how to maximize
      locality. (Blocking/ Tiling)
    \end{enumerate}
  \item Cache-Aware programming
    \begin{enumerate}
    \item Hardware stream/stride pre-fetching
    \item When to software pre-fetch
    \item Blocking/ Tiling
    \end{enumerate}
  \item Virtual Memory (VM):
    \begin{enumerate}
    \item Indirection: Adding an intermediate interface that controls endpoints
    \item Use of indirection in VM: Virtual Memory $\rightarrow$ OS
      $\rightarrow$ physical memory.This eliminates the need for
      explicit overlaying in programs.
    \item Advantages of VM.
      \begin{enumerate}
      \item Larger than memory processes: We can store ``memory'' on
        disk via paging. Also because its virtual we can say we have
        ``n'' bits of mem but only have to allocate as much as needed.
      \item Identical/Conflicting address mapping: 2 programs can both
        map to the same address because the OS/PageTable deals with
        actual mapping. i.e. sure you can have $0x4004$ (hands you $0xff0f00$)
      \item Sand-boxing processes: The page table has permission bits
        that tells the OS who can touch the cookie jar.
      \item Controlled sharing between processes: If two addresses
        need to share, it can be done under adult(OS) super vision
        because of the page table's permission bits.
      \end{enumerate}
    \item Page table: Look up table that maps virtual to physical memory
    \item Translation look-aside buffer (TLB): Looking up the page
      table translations sucks....so we cache it...aka TLB 
    \item Hierarchical page table: Page Directories $\rightarrow$ Page
      Entry $\rightarrow$ Page Table $\rightarrow$ Address . (note
      saves memory because not all Entries and Directories are
      filled.\\
      P.S. page tables should be page sizes so they themselves can be
      looked up easily....
    \item Page faults(and its affect on virtual memory design): Miss
      in the TLB, (resulting in memory seak for page). Miss in the
      cache to get page/addresses. ew....  
    \end{enumerate}
  \item Hard Disks
    \begin{enumerate}
    \item Platter: Cyclinder that houses magnetic particles organized
      in tracks:
    \item Head: Reads/Writes magnetics particles (moves by arm)
    \item Track: Circular collection of sectors
    \item Sector: Partition of Track where data is actually stored
    \item 
      \begin{equation}
        readWriteTime = Seek+RotDely +sectorRead\cdot sectors
      \end{equation}

    \item Random vs Sequential read/write performance
    \item Solid state disks (SSD): Flash memory based, everything but
      random writes it very fast. (writes are slow due to having to
      clear the whole block before to write)
    \end{enumerate}
    \begin{enumerate}
    \item Error Correcting Codes
      \begin{enumerate}
      \item Error detection vs. Error correction:
        \begin{enumerate}
        \item Error Detection is the ability to detect that somehow a
          bit (or more) has been flipped.
        \item Error Correction is the ability to detect an error has
          occurred and guess what it might have been before being flipped.
        \item Relationship: Error Detection is achievable with fewer
          bits and can be done to a further level of error (more bits
          flipped) but does not correct. Correction gives up the
          ability to detect as far but adds the ability to guess
          initial states.
        \end{enumerate}
      \item Parity: Extra bit that is the result of xor the bitstring
        (odd or even). Parity is recomputed when reading Ram and
        checked. If $P_{stored} \neq P_{computed}$ an error has occured.
      \item Hamming distance: Minimum path of states to no longer
        detect/correct errors.
      \item SECDED: Single-bit Error Correction, Double-bit Error
        Detection. Standard in ECC protect SDRAM.
      \end{enumerate}
    \item Writing code to analyze cache with a hardware pre-fetcher
      \begin{enumerate}
      \item Needs to loop such that the stride can't be predicted by
        prefecter (link list is a good canidate)
      \end{enumerate}
    \end{enumerate}
  \end{enumerate}
  \textbf{Post-Exam 3}
  \begin{enumerate}
  \item Multi-core
    \begin{enumerate}
    \item Definition: A group of processors with a shared memory
      system. Used because higher clock speed is very power
      hungry/impractical and better exposes parallelism. 
    \item Thread: A scheduled unit of processing. Can interact with
      other threads or remain isolated.
    \item Cache Coherence (MSI model):
      \begin{enumerate}
      \item Modified: Only one copy allowed. Can be changed.
      \item Shared: Many copies allowed. Cannot be changed.
      \item Invalid: No copies (except in main memory).
      \item GetS: Requests a shared version of memory. Demotes
        Modified states to Shared.
      \item GetX: Requests an exclusive(modifiable) version of
        memory. All other versions Invalidated.
      \item Upgrade: Promote Shared state to Modified. Invalidates all
        other versions.
      \item source data: if demotion from modified to shared, occurs a
        processor can source the data to another processor instead of
        getting it from main memory (which is slower.)
      \item write back: if no processor will have a copy of the
        modified data then it must be written back to main memory.
      \item Share: Accessing the same data in multiple processors
        requiring numerous state changes (slow)
      \item False sharing: Accessing data in the same cache block
        effectively forcing the processors to fight over exclusive
        state. *avoided by moving data to another cache block (padding
        or some other means)
      \end{enumerate}

    \item Sequential Consistency: Strongest Memory Access model. All
      instructions are sequentially accessed within their own thread
      and thus are mearly interleaved with other threads if they exist.(Slow)
    \item Locking/Blocking/Fencing: Because sequential consistency
      isn't guarenteed (or even the default on most systems) one can
      force critical sections of code into sequential consistency via
      'fencing' off the section. This is supported in hardware via the
      Compare-and-swap (CAS) call. Locking/Block is a common
      implementation of this.
    \end{enumerate}

  \item Exploiting Parallelism
    \begin{enumerate}
    \item Vector Processing: Taking a single register and treating it
      as multiple piece of data so that arithmetic/logic can be performed on
      each section simulatinously. Single Instuction Multiple Data
      (SIMD) is an example implentation given by intel.
    \item Speed Ups via vector processing: limited by $\frac{register
      Size}{dataSize}$ i.e. how many independent elements can be done
      in one instruction and the cost of syncing them (ideally none)
    \item Fork-Join parallelism model: Have a master thread and during
      parallelisable sections of code, 'fork' of independent threads
      and then sync when done to the master thread.
    \item Speed Ups via multi-core parellism: Ignoring cache speed
      ups (may result in super linear growth), one expects linear
      growth with the number of processors minus the overhead of
      splitting/syncing. Again limited by Amdahl's Law.
    \end{enumerate}

  \end{enumerate}
\end{document}
% LocalWords:  LRU Associatity associatity
